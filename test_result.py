import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import os
from tqdm import tqdm
import warnings
from scipy import stats
from sklearn.metrics import roc_auc_score, precision_recall_curve, classification_report
import torch
import torch.nn.functional as F

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾å½¢æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

class CompleteCNNAnalysisPipeline:
    """
    å®Œæ•´çš„CNNæ¨¡å‹æ€§èƒ½åˆ†ææµæ°´çº¿ - ä¸¥æ ¼å¤ç°ã€Š(Re-)Imag(in)ing Price Trendsã€‹è®ºæ–‡æŒ‡æ ‡
    æ•°æ®èŒƒå›´ï¼š2000-2019å¹´
    """
    
    def __init__(self, data_dir, output_dir="./cnn_complete_analysis_2000_2019"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.yearly_data = {}
        self.combined_data = None
        self.results = {
            'yearly_metrics': {'classification': [], 'portfolio': []},
            'pooled_metrics': {},
            'cross_year_summary': {}
        }
        self.years_range = range(2000, 2020)  # 2000-2019å¹´
    
    def load_all_years_data(self):
        """åŠ è½½2000-2019å¹´æ‰€æœ‰æ•°æ®"""
        print("åŠ è½½2000-2019å¹´æ•°æ®...")
        loaded_years = []
        
        for year in tqdm(self.years_range, desc="åŠ è½½å¹´ä»½æ•°æ®"):
            # åŒ¹é…æ–‡ä»¶æ¨¡å¼
            file_patterns = [
                f"*{year}*with_cnn.feather",
                f"*{year}*labels_w_delay_with_cnn.feather", 
                f"*{year}*cnn.feather"
            ]
            
            file_path = None
            for pattern in file_patterns:
                matching_files = list(self.data_dir.glob(pattern))
                if matching_files:
                    file_path = matching_files[0]
                    break
            
            if file_path and file_path.exists():
                try:
                    df = pd.read_feather(file_path)
                    # ç¡®ä¿åŒ…å«å¿…è¦åˆ—
                    required_cols = ['Date', 'cnn_prob_20d', 'Ret_20d']
                    if all(col in df.columns for col in required_cols):
                        df['Date'] = pd.to_datetime(df['Date'])
                        df['Year'] = df['Date'].dt.year
                        self.yearly_data[year] = df
                        loaded_years.append(year)
                        print(f"âœ“ æˆåŠŸåŠ è½½ {year} å¹´æ•°æ®: {len(df):,} æ ·æœ¬")
                    else:
                        print(f"âš  {year} å¹´æ•°æ®ç¼ºå°‘å¿…è¦åˆ—")
                except Exception as e:
                    print(f"âœ— åŠ è½½ {year} å¹´æ•°æ®å¤±è´¥: {e}")
            else:
                print(f"âš  æœªæ‰¾åˆ° {year} å¹´æ•°æ®æ–‡ä»¶")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®ç”¨äºæ•´ä½“è®¡ç®—
        if self.yearly_data:
            self.combined_data = pd.concat(self.yearly_data.values(), ignore_index=True)
            print(f"\næ•°æ®åŠ è½½æ‘˜è¦:")
            print(f"æˆåŠŸåŠ è½½å¹´ä»½: {len(loaded_years)}å¹´ ({min(loaded_years)}-{max(loaded_years)})")
            print(f"æ€»æ ·æœ¬æ•°: {len(self.combined_data):,}")
            print(f"æ—¶é—´èŒƒå›´: {self.combined_data['Date'].min()} åˆ° {self.combined_data['Date'].max()}")
        else:
            print("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")
    
    def calculate_comprehensive_correlations(self, df):
        """
        è®¡ç®—å®Œæ•´çš„ç›¸å…³æ€§æŒ‡æ ‡ï¼ˆå¤ç°åŸæ–‡è¡¨2ï¼‰
        åŒ…å«Spearmanå’ŒPearsonç›¸å…³ç³»æ•°
        """
        df_clean = df.dropna(subset=['cnn_prob_20d', 'Ret_20d'])
        
        if len(df_clean) < 2:
            return {}
        
        # 1. æ•´ä½“ç›¸å…³æ€§
        spearman_overall = df_clean['cnn_prob_20d'].corr(df_clean['Ret_20d'], method='spearman')
        pearson_overall = df_clean['cnn_prob_20d'].corr(df_clean['Ret_20d'], method='pearson')
        
        # 2. æ¨ªæˆªé¢ç›¸å…³æ€§ï¼ˆæŒ‰æ—¶é—´ç‚¹ï¼‰
        def cross_sectional_corr(group, method='spearman'):
            if len(group) < 2:
                return np.nan
            return group['cnn_prob_20d'].corr(group['Ret_20d'], method=method)
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„æ¨ªæˆªé¢ç›¸å…³æ€§
        spearman_cross = df_clean.groupby('Date').apply(cross_sectional_corr, method='spearman')
        pearson_cross = df_clean.groupby('Date').apply(cross_sectional_corr, method='pearson')
        
        # 3. ä¿¡æ¯ç³»æ•°(Information Coefficient)åˆ†æ
        ic_results = self.calculate_information_coefficient(df_clean)
        
        return {
            'spearman': {
                'overall': spearman_overall,
                'cross_sectional_mean': spearman_cross.mean(),
                'cross_sectional_std': spearman_cross.std(),
                'cross_sectional_ts': spearman_cross,
                'significant': abs(spearman_overall) > 0.05
            },
            'pearson': {
                'overall': pearson_overall,
                'cross_sectional_mean': pearson_cross.mean(),
                'cross_sectional_std': pearson_cross.std(),
                'cross_sectional_ts': pearson_cross,
                'significant': abs(pearson_overall) > 0.05
            },
            'information_coefficient': ic_results,
            'sample_size': len(df_clean)
        }
    
    def calculate_information_coefficient(self, df):
        """è®¡ç®—ä¿¡æ¯ç³»æ•°(IC)åŠç›¸å…³ç»Ÿè®¡é‡"""
        if len(df) < 2:
            return {}
        
        # æŒ‰æ—¶é—´ç‚¹è®¡ç®—ICï¼ˆä½¿ç”¨Spearmanï¼‰
        def period_ic(group):
            if len(group) < 2:
                return np.nan
            return group['cnn_prob_20d'].corr(group['Ret_20d'], method='spearman')
        
        ic_series = df.groupby('Date').apply(period_ic).dropna()
        
        if len(ic_series) == 0:
            return {}
        
        # ICç»Ÿè®¡é‡
        mean_ic = ic_series.mean()
        std_ic = ic_series.std()
        ic_ir = mean_ic / std_ic if std_ic > 0 else 0
        positive_ratio = (ic_series > 0).mean()
        
        # ICçš„tæ£€éªŒ
        if len(ic_series) > 1 and std_ic > 0:
            t_stat, p_value = stats.ttest_1samp(ic_series, 0)
        else:
            t_stat, p_value = 0, 1.0
        
        return {
            'mean_ic': mean_ic,
            'std_ic': std_ic,
            'ic_ir': ic_ir,  # ä¿¡æ¯æ¯”ç‡
            'positive_ratio': positive_ratio,
            't_statistic': t_stat,
            'p_value': p_value,
            'significant_5pct': p_value < 0.05,
            'significant_1pct': p_value < 0.01,
            'ic_time_series': ic_series,
            'periods': len(ic_series)
        }
    
    def calculate_classification_metrics(self, df):
        """è®¡ç®—å®Œæ•´çš„åˆ†ç±»æŒ‡æ ‡"""
        df_clean = df.dropna(subset=['cnn_prob_20d', 'Ret_20d'])
        
        if len(df_clean) == 0:
            return {}
        
        # æ–¹å‘é¢„æµ‹
        df_clean = df_clean.copy()
        df_clean['pred_direction'] = (df_clean['cnn_prob_20d'] > 0.5).astype(int)
        df_clean['actual_direction'] = (df_clean['Ret_20d'] > 0).astype(int)
        
        # æ··æ·†çŸ©é˜µ
        tp = ((df_clean['pred_direction'] == 1) & (df_clean['actual_direction'] == 1)).sum()
        fp = ((df_clean['pred_direction'] == 1) & (df_clean['actual_direction'] == 0)).sum()
        fn = ((df_clean['pred_direction'] == 0) & (df_clean['actual_direction'] == 1)).sum()
        tn = ((df_clean['pred_direction'] == 0) & (df_clean['actual_direction'] == 0)).sum()
        
        total = tp + fp + fn + tn
        if total == 0:
            return {}
        
        # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
        accuracy = (tp + tn) / total
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # AUC-ROC
        try:
            auc_roc = roc_auc_score(df_clean['actual_direction'], df_clean['cnn_prob_20d'])
        except:
            auc_roc = 0.5
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_roc': auc_roc,
            'true_positives': tp,
            'false_positives': fp,
            'true_negatives': tn,
            'false_negatives': fn,
            'total_samples': total,
            'positive_ratio': df_clean['actual_direction'].mean()  # æ­£æ ·æœ¬æ¯”ä¾‹
        }
    
    def calculate_portfolio_performance(self, df, n_deciles=10):
        """è®¡ç®—æŠ•èµ„ç»„åˆè¡¨ç°ï¼ˆå¤ç°åŸæ–‡è¡¨3ï¼‰"""
        df_clean = df.dropna(subset=['cnn_prob_20d', 'Ret_20d'])
        
        if len(df_clean) < n_deciles * 5:  # ç¡®ä¿è¶³å¤Ÿæ ·æœ¬
            return None
        
        # æŒ‰é¢„æµ‹æ¦‚ç‡åˆ†ååˆ†ä½
        df_clean = df_clean.copy()
        df_clean['decile'] = df_clean.groupby('Date')['cnn_prob_20d'].transform(
            lambda x: pd.qcut(x, n_deciles, labels=False, duplicates='drop')
        )
        
        # ç­‰æƒé‡ç»„åˆæ”¶ç›Š
        decile_returns_ew = df_clean.groupby(['Date', 'decile'])['Ret_20d'].mean().unstack()
        
        # å¸‚å€¼æƒé‡ç»„åˆæ”¶ç›Š
        def value_weighted_return(group):
            if 'MarketCap' in group.columns:
                total_mcap = group['MarketCap'].sum()
                if total_mcap > 0:
                    return np.average(group['Ret_20d'], weights=group['MarketCap'])
            return group['Ret_20d'].mean()
        
        decile_returns_vw = df_clean.groupby(['Date', 'decile']).apply(value_weighted_return).unstack()
        
        # H-Lç»„åˆï¼ˆå¤šç©ºç­–ç•¥ï¼‰
        if n_deciles-1 in decile_returns_ew.columns and 0 in decile_returns_ew.columns:
            hl_returns_ew = decile_returns_ew[n_deciles-1] - decile_returns_ew[0]
            hl_returns_vw = decile_returns_vw[n_deciles-1] - decile_returns_vw[0]
        else:
            # å¦‚æœåˆ†ä½ä¸å®Œæ•´ï¼Œä½¿ç”¨æœ€é«˜æœ€ä½åˆ†ä½
            available_deciles = [col for col in decile_returns_ew.columns if not pd.isna(col)]
            if len(available_deciles) >= 2:
                high_decile = max(available_deciles)
                low_decile = min(available_deciles)
                hl_returns_ew = decile_returns_ew[high_decile] - decile_returns_ew[low_decile]
                hl_returns_vw = decile_returns_vw[high_decile] - decile_returns_vw[low_decile]
            else:
                return None
        
        # è®¡ç®—å¤æ™®æ¯”ç‡å’Œç»Ÿè®¡æ˜¾è‘—æ€§
        sharpe_ew, sig_ew = self.calculate_annualized_sharpe(hl_returns_ew)
        sharpe_vw, sig_vw = self.calculate_annualized_sharpe(hl_returns_vw)
        
        return {
            'sharpe_ratio_ew': sharpe_ew,
            'sharpe_ratio_vw': sharpe_vw,
            'hl_return_mean_ew': hl_returns_ew.mean(),
            'hl_return_std_ew': hl_returns_ew.std(),
            'hl_cumulative_ew': (1 + hl_returns_ew).prod() - 1,
            'significance_ew': sig_ew,
            'significance_vw': sig_vw,
            'decile_returns_ew': decile_returns_ew,
            'decile_returns_vw': decile_returns_vw,
            'sample_size': len(df_clean),
            'periods': len(hl_returns_ew)
        }
    
    def calculate_annualized_sharpe(self, returns, periods_per_year=12):
        """è®¡ç®—å¹´åŒ–å¤æ™®æ¯”ç‡åŠç»Ÿè®¡æ˜¾è‘—æ€§"""
        if len(returns) < 2 or returns.std() == 0:
            return 0, {'significant': False, 't_stat': 0, 'p_value': 1.0}
        
        annual_return = returns.mean() * periods_per_year
        annual_volatility = returns.std() * np.sqrt(periods_per_year)
        sharpe_ratio = annual_return / annual_volatility
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        t_stat, p_value = stats.ttest_1samp(returns, 0)
        significant_5pct = p_value < 0.05
        significant_1pct = p_value < 0.01
        
        significance = {
            't_statistic': t_stat,
            'p_value': p_value,
            'significant_5pct': significant_5pct,
            'significant_1pct': significant_1pct
        }
        
        return sharpe_ratio, significance
    
    def run_complete_analysis(self):
        """æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("="*70)
        print("å¼€å§‹2000-2019å¹´CNNæ¨¡å‹æ€§èƒ½å®Œæ•´åˆ†æ")
        print("="*70)
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_all_years_data()
            
            if not self.yearly_data:
                print("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œåˆ†æç»ˆæ­¢")
                return
            
            # 2. åˆ†å¹´è®¡ç®—å„é¡¹æŒ‡æ ‡
            print("\næ­¥éª¤1: åˆ†å¹´è®¡ç®—åˆ†ç±»å’Œç»„åˆæŒ‡æ ‡...")
            self.calculate_yearly_metrics()
            
            # 3. æ•´ä½“è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡
            print("\næ­¥éª¤2: æ•´ä½“è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡...")
            self.calculate_pooled_correlations()
            
            # 4. æ•´ä½“è®¡ç®—åˆ†ç±»æŒ‡æ ‡
            print("\næ­¥éª¤3: æ•´ä½“è®¡ç®—åˆ†ç±»æŒ‡æ ‡...")
            self.calculate_pooled_classification()
            
            # 5. è®¡ç®—è·¨å¹´æ±‡æ€»ç»Ÿè®¡
            print("\næ­¥éª¤4: è®¡ç®—è·¨å¹´æ±‡æ€»...")
            self.calculate_cross_year_summary()
            
            # 6. ä¿å­˜æ‰€æœ‰ç»“æœ
            print("\næ­¥éª¤5: ä¿å­˜ç»“æœ...")
            self.save_comprehensive_results()
            
            # 7. ç”Ÿæˆå¯è§†åŒ–
            print("\næ­¥éª¤6: ç”Ÿæˆå¯è§†åŒ–...")
            self.create_comprehensive_visualizations()
            
            # 8. æ‰“å°æŠ¥å‘Š
            print("\næ­¥éª¤7: ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            self.print_detailed_report()
            
            print("\n" + "="*70)
            print("åˆ†æå®Œæˆï¼")
            print("="*70)
            
        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def calculate_yearly_metrics(self):
        """è®¡ç®—åˆ†å¹´æŒ‡æ ‡"""
        yearly_classification = []
        yearly_portfolio = []
        
        for year, df in tqdm(self.yearly_data.items(), desc="åˆ†å¹´è®¡ç®—"):
            # åˆ†ç±»æŒ‡æ ‡
            class_metrics = self.calculate_classification_metrics(df)
            if class_metrics:
                class_metrics['year'] = year
                yearly_classification.append(class_metrics)
            
            # æŠ•èµ„ç»„åˆæŒ‡æ ‡
            port_metrics = self.calculate_portfolio_performance(df)
            if port_metrics:
                port_metrics['year'] = year
                yearly_portfolio.append(port_metrics)
        
        self.results['yearly_metrics']['classification'] = yearly_classification
        self.results['yearly_metrics']['portfolio'] = yearly_portfolio
    
    def calculate_pooled_correlations(self):
        """è®¡ç®—æ•´ä½“ç›¸å…³æ€§æŒ‡æ ‡"""
        if self.combined_data is not None:
            corr_metrics = self.calculate_comprehensive_correlations(self.combined_data)
            self.results['pooled_metrics']['correlations'] = corr_metrics
    
    def calculate_pooled_classification(self):
        """è®¡ç®—æ•´ä½“åˆ†ç±»æŒ‡æ ‡"""
        if self.combined_data is not None:
            class_metrics = self.calculate_classification_metrics(self.combined_data)
            self.results['pooled_metrics']['classification'] = class_metrics
    
    def calculate_cross_year_summary(self):
        """è®¡ç®—è·¨å¹´æ±‡æ€»ç»Ÿè®¡"""
        class_df = pd.DataFrame(self.results['yearly_metrics']['classification'])
        port_df = pd.DataFrame([m for m in self.results['yearly_metrics']['portfolio'] if m is not None])
        
        if class_df.empty:
            self.results['cross_year_summary'] = {}
            return
        
        summary = {
            # æ ·æœ¬ç»Ÿè®¡
            'total_years': len(class_df),
            'total_samples': class_df['total_samples'].sum() if 'total_samples' in class_df.columns else 0,
            'mean_samples_per_year': class_df['total_samples'].mean() if 'total_samples' in class_df.columns else 0,
            
            # åˆ†ç±»æŒ‡æ ‡æ±‡æ€»
            'mean_accuracy': class_df['accuracy'].mean(),
            'std_accuracy': class_df['accuracy'].std(),
            'mean_precision': class_df['precision'].mean() if 'precision' in class_df.columns else 0,
            'mean_recall': class_df['recall'].mean() if 'recall' in class_df.columns else 0,
            'mean_f1_score': class_df['f1_score'].mean() if 'f1_score' in class_df.columns else 0,
            'mean_auc_roc': class_df['auc_roc'].mean() if 'auc_roc' in class_df.columns else 0,
            
            # æ—¶é—´è¶‹åŠ¿
            'accuracy_trend': self.calculate_trend(class_df, 'accuracy'),
            'f1_trend': self.calculate_trend(class_df, 'f1_score') if 'f1_score' in class_df.columns else 0,
        }
        
        # æŠ•èµ„ç»„åˆæŒ‡æ ‡æ±‡æ€»
        if not port_df.empty:
            summary.update({
                'mean_sharpe_ew': port_df['sharpe_ratio_ew'].mean(),
                'mean_sharpe_vw': port_df['sharpe_ratio_vw'].mean(),
                'std_sharpe_ew': port_df['sharpe_ratio_ew'].std(),
                'sharpe_trend': self.calculate_trend(port_df, 'sharpe_ratio_ew'),
                'significant_years_5pct': port_df['significance_ew'].apply(
                    lambda x: x.get('significant_5pct', False) if isinstance(x, dict) else False
                ).sum() if 'significance_ew' in port_df.columns else 0
            })
        
        self.results['cross_year_summary'] = summary
    
    def calculate_trend(self, df, column):
        """è®¡ç®—æ—¶é—´è¶‹åŠ¿æ–œç‡"""
        if column not in df.columns or len(df) < 2:
            return 0
        x = np.arange(len(df))
        y = df[column].values
        mask = ~np.isnan(y)
        if mask.sum() < 2:
            return 0
        slope = np.polyfit(x[mask], y[mask], 1)[0]
        return slope
    
    def save_comprehensive_results(self):
        """ä¿å­˜å®Œæ•´ç»“æœåˆ°æ–‡ä»¶"""
        print("ä¿å­˜åˆ†æç»“æœ...")
        
        try:
            # 1. ä¿å­˜åˆ†å¹´åˆ†ç±»æŒ‡æ ‡
            if self.results['yearly_metrics']['classification']:
                class_df = pd.DataFrame(self.results['yearly_metrics']['classification'])
                class_df.to_csv(self.output_dir / "yearly_classification_metrics.csv", 
                              index=False, float_format='%.6f')
                print("âœ“ åˆ†å¹´åˆ†ç±»æŒ‡æ ‡å·²ä¿å­˜")
            
            # 2. ä¿å­˜åˆ†å¹´ç»„åˆæŒ‡æ ‡
            valid_portfolio = [m for m in self.results['yearly_metrics']['portfolio'] if m is not None]
            if valid_portfolio:
                port_df = pd.DataFrame(valid_portfolio)
                port_df.to_csv(self.output_dir / "yearly_portfolio_metrics.csv", 
                              index=False, float_format='%.6f')
                print("âœ“ åˆ†å¹´ç»„åˆæŒ‡æ ‡å·²ä¿å­˜")
            
            # 3. ä¿å­˜æ•´ä½“ç›¸å…³æ€§æŒ‡æ ‡
            if self.results['pooled_metrics'].get('correlations'):
                corr_data = {}
                for corr_type, metrics in self.results['pooled_metrics']['correlations'].items():
                    if isinstance(metrics, dict):
                        for key, value in metrics.items():
                            if not isinstance(value, (pd.Series, pd.DataFrame)) and not isinstance(value, dict):
                                corr_data[f'{corr_type}_{key}'] = value
                
                if corr_data:
                    pd.DataFrame([corr_data]).to_csv(self.output_dir / "correlation_metrics.csv", 
                                                    index=False, float_format='%.6f')
                    print("âœ“ ç›¸å…³æ€§æŒ‡æ ‡å·²ä¿å­˜")
            
            # 4. ä¿å­˜æ•´ä½“åˆ†ç±»æŒ‡æ ‡
            if self.results['pooled_metrics'].get('classification'):
                class_metrics = self.results['pooled_metrics']['classification']
                pd.DataFrame([class_metrics]).to_csv(self.output_dir / "pooled_classification_metrics.csv", 
                                                    index=False, float_format='%.6f')
                print("âœ“ æ•´ä½“åˆ†ç±»æŒ‡æ ‡å·²ä¿å­˜")
            
            # 5. ä¿å­˜è·¨å¹´æ±‡æ€»
            if self.results['cross_year_summary']:
                summary_df = pd.DataFrame([self.results['cross_year_summary']])
                summary_df.to_csv(self.output_dir / "cross_year_summary.csv", 
                                index=False, float_format='%.6f')
                print("âœ“ è·¨å¹´æ±‡æ€»å·²ä¿å­˜")
            
            # 6. ä¿å­˜ICæ—¶é—´åºåˆ—
            ic_ts = self.results['pooled_metrics'].get('correlations', {}).get('information_coefficient', {}).get('ic_time_series')
            if ic_ts is not None:
                ic_ts.to_csv(self.output_dir / "information_coefficient_timeseries.csv", 
                           index=True, float_format='%.6f')
                print("âœ“ ICæ—¶é—´åºåˆ—å·²ä¿å­˜")
            
            print(f"âœ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
            
        except Exception as e:
            print(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
    
    def create_comprehensive_visualizations(self):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""
        print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        try:
            # 1. å¹´åº¦å‡†ç¡®ç‡è¶‹åŠ¿å›¾
            if self.results['yearly_metrics']['classification']:
                class_df = pd.DataFrame(self.results['yearly_metrics']['classification'])
                plt.figure(figsize=(12, 6))
                plt.plot(class_df['year'], class_df['accuracy'], 'o-', linewidth=2, markersize=6)
                plt.xlabel('å¹´ä»½')
                plt.ylabel('åˆ†ç±»å‡†ç¡®ç‡')
                plt.title('CNNæ¨¡å‹å¹´åº¦åˆ†ç±»å‡†ç¡®ç‡è¶‹åŠ¿ (2000-2019)')
                plt.grid(True, alpha=0.3)
                plt.xticks(class_df['year'][::2])
                plt.savefig(self.output_dir / "yearly_accuracy_trend.png", dpi=300, bbox_inches='tight')
                plt.close()
                print("âœ“ å‡†ç¡®ç‡è¶‹åŠ¿å›¾å·²ç”Ÿæˆ")
            
            # 2. å¹´åº¦å¤æ™®æ¯”ç‡è¶‹åŠ¿
            valid_portfolio = [m for m in self.results['yearly_metrics']['portfolio'] if m is not None]
            if valid_portfolio:
                port_df = pd.DataFrame(valid_portfolio)
                if not port_df.empty and 'sharpe_ratio_ew' in port_df.columns:
                    plt.figure(figsize=(12, 6))
                    plt.plot(port_df['year'], port_df['sharpe_ratio_ew'], 's-', linewidth=2, markersize=6, label='ç­‰æƒé‡')
                    if 'sharpe_ratio_vw' in port_df.columns:
                        plt.plot(port_df['year'], port_df['sharpe_ratio_vw'], '^-', linewidth=2, markersize=6, label='å¸‚å€¼æƒé‡')
                    plt.xlabel('å¹´ä»½')
                    plt.ylabel('å¹´åŒ–å¤æ™®æ¯”ç‡')
                    plt.title('H-Lç»„åˆå¹´åº¦å¤æ™®æ¯”ç‡è¶‹åŠ¿ (2000-2019)')
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    plt.savefig(self.output_dir / "yearly_sharpe_trend.png", dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ å¤æ™®æ¯”ç‡è¶‹åŠ¿å›¾å·²ç”Ÿæˆ")
            
            # 3# 3. ä¿¡æ¯ç³»æ•°(IC)æ—¶é—´åºåˆ—å›¾
            if 'information_coefficient' in self.results['pooled_metrics'].get('correlations', {}):
                ic_data = self.results['pooled_metrics']['correlations']['information_coefficient']
                if 'ic_time_series' in ic_data and ic_data['ic_time_series'] is not None:
                    ic_series = ic_data['ic_time_series']
                    plt.figure(figsize=(12, 6))
                    plt.plot(ic_series.index, ic_series.values, linewidth=1, alpha=0.7)
                    plt.axhline(y=ic_series.mean(), color='r', linestyle='--', 
                               label=f'å‡å€¼: {ic_series.mean():.3f}')
                    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
                    plt.xlabel('æ—¥æœŸ')
                    plt.ylabel('ä¿¡æ¯ç³»æ•° (IC)')
                    plt.title('ä¿¡æ¯ç³»æ•°æ—¶é—´åºåˆ— (2000-2019)', fontsize=14)
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    plt.tight_layout()
                    plt.savefig(self.output_dir / "information_coefficient_timeseries.png", 
                               dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ ICæ—¶é—´åºåˆ—å›¾å·²ç”Ÿæˆ")

            # 4. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾
            if self.combined_data is not None and 'cnn_prob_20d' in self.combined_data.columns:
                probs = self.combined_data['cnn_prob_20d'].dropna()
                if len(probs) > 0:
                    plt.figure(figsize=(10, 6))
                    plt.hist(probs, bins=50, alpha=0.7, edgecolor='black', density=True)
                    plt.axvline(x=0.5, color='r', linestyle='--', label='å†³ç­–è¾¹ç•Œ (0.5)')
                    plt.xlabel('CNNé¢„æµ‹æ¦‚ç‡')
                    plt.ylabel('å¯†åº¦')
                    plt.title('CNNé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ (2000-2019å…¨æ ·æœ¬)', fontsize=14)
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    plt.tight_layout()
                    plt.savefig(self.output_dir / "prediction_probability_distribution.png", 
                               dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå›¾å·²ç”Ÿæˆ")

            # 5. åˆ†ä½æ•°ç»„åˆæ”¶ç›Šçƒ­åŠ›å›¾
            if self.results['yearly_metrics']['portfolio']:
                # æå–æ‰€æœ‰å¹´ä»½çš„åˆ†ä½æ•°æ”¶ç›Šæ•°æ®
                all_decile_returns = []
                years = []
                for result in self.results['yearly_metrics']['portfolio']:
                    if result and 'decile_returns_ew' in result:
                        decile_returns = result['decile_returns_ew'].mean()  # æ—¶é—´åºåˆ—å¹³å‡
                        all_decile_returns.append(decile_returns)
                        years.append(result['year'])
                
                if all_decile_returns and len(all_decile_returns) > 0:
                    returns_df = pd.DataFrame(all_decile_returns, index=years)
                    returns_df = returns_df.sort_index()
                    
                    plt.figure(figsize=(14, 10))
                    sns.heatmap(returns_df.T, annot=True, fmt=".3f", cmap="RdYlGn", 
                               cbar_kws={'label': 'å¹³å‡æ”¶ç›Š'}, center=0)
                    plt.xlabel('å¹´ä»½')
                    plt.ylabel('åˆ†ä½æ•°')
                    plt.title('åˆ†ä½æ•°ç»„åˆå¹´å¹³å‡æ”¶ç›Šçƒ­åŠ›å›¾ (ç­‰æƒé‡)', fontsize=14)
                    plt.tight_layout()
                    plt.savefig(self.output_dir / "decile_returns_heatmap.png", 
                               dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ åˆ†ä½æ•°æ”¶ç›Šçƒ­åŠ›å›¾å·²ç”Ÿæˆ")

            # 6. H-Lç»„åˆç´¯ç§¯æ”¶ç›Šæ›²çº¿å›¾
            if self.results['yearly_metrics']['portfolio']:
                # è®¡ç®—ä»£è¡¨æ€§å¹´ä»½çš„ç´¯ç§¯æ”¶ç›Š
                representative_years = [2000, 2005, 2010, 2015]  # é€‰æ‹©ä»£è¡¨æ€§å¹´ä»½
                fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                axes = axes.ravel()
                
                plotted_count = 0
                for i, year in enumerate(representative_years):
                    # æŸ¥æ‰¾è¯¥å¹´çš„ç»„åˆæ•°æ®
                    year_result = next((r for r in self.results['yearly_metrics']['portfolio'] 
                                      if r and r.get('year') == year), None)
                    
                    if year_result and 'decile_returns_ew' in year_result:
                        decile_returns = year_result['decile_returns_ew']
                        if len(decile_returns.columns) >= 2:
                            # è®¡ç®—H-Lç»„åˆæ”¶ç›Š
                            high_decile = decile_returns.columns[-1]
                            low_decile = decile_returns.columns[0]
                            hl_returns = decile_returns[high_decile] - decile_returns[low_decile]
                            cumulative_returns = (1 + hl_returns).cumprod()
                            
                            if i < len(axes):
                                axes[i].plot(cumulative_returns.index, cumulative_returns.values, 
                                           linewidth=2, color=f'C{i}')
                                axes[i].set_title(f'{year}å¹´ H-Lç»„åˆç´¯ç§¯æ”¶ç›Š', fontsize=12)
                                axes[i].set_xlabel('æ—¥æœŸ')
                                axes[i].set_ylabel('ç´¯ç§¯æ”¶ç›Š')
                                axes[i].grid(True, alpha=0.3)
                                axes[i].tick_params(axis='x', rotation=45)
                                plotted_count += 1
                
                if plotted_count > 0:
                    # ç§»é™¤ç©ºçš„å­å›¾
                    for j in range(plotted_count, 4):
                        if j < len(axes):
                            fig.delaxes(axes[j])
                    
                    plt.tight_layout()
                    plt.savefig(self.output_dir / "hl_strategy_cumulative_returns.png", 
                               dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ H-Lç»„åˆç´¯ç§¯æ”¶ç›Šå›¾å·²ç”Ÿæˆ")

            # 7. ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆä¸åŒæŒ‡æ ‡é—´ï¼‰
            correlation_data = {}
            if self.results['yearly_metrics']['classification']:
                class_df = pd.DataFrame(self.results['yearly_metrics']['classification'])
                # æå–æ•°å€¼å‹æŒ‡æ ‡
                numeric_cols = class_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 1:
                    correlation_matrix = class_df[numeric_cols].corr()
                    
                    plt.figure(figsize=(10, 8))
                    sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", 
                               center=0, square=True)
                    plt.title('å¹´åº¦æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=14)
                    plt.tight_layout()
                    plt.savefig(self.output_dir / "yearly_metrics_correlation_heatmap.png", 
                               dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ç”Ÿæˆ")

            # 8. å¹´åº¦æ ·æœ¬é‡åˆ†å¸ƒå›¾
            if self.results['yearly_metrics']['classification']:
                class_df = pd.DataFrame(self.results['yearly_metrics']['classification'])
                if 'total_samples' in class_df.columns:
                    plt.figure(figsize=(12, 6))
                    plt.bar(class_df['year'], class_df['total_samples'], 
                           alpha=0.7, color='skyblue', edgecolor='black')
                    plt.xlabel('å¹´ä»½')
                    plt.ylabel('æ ·æœ¬æ•°é‡')
                    plt.title('å¹´åº¦æ ·æœ¬é‡åˆ†å¸ƒ (2000-2019)', fontsize=14)
                    plt.xticks(class_df['year'][::2])
                    plt.grid(True, alpha=0.3, axis='y')
                    plt.tight_layout()
                    plt.savefig(self.output_dir / "yearly_sample_distribution.png", 
                               dpi=300, bbox_inches='tight')
                    plt.close()
                    print("âœ“ æ ·æœ¬é‡åˆ†å¸ƒå›¾å·²ç”Ÿæˆ")

            print("ğŸ¨ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")

        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("CNNæ¨¡å‹æ€§èƒ½è¯¦ç»†åˆ†ææŠ¥å‘Š (2000-2019)")
        print("="*80)
        
        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        summary = self.results['cross_year_summary']
        print(f"\n1. æ ·æœ¬ç»Ÿè®¡æ¦‚è¦:")
        print(f"   è¦†ç›–å¹´ä»½: {summary.get('total_years', 0)}å¹´")
        print(f"   æ€»æ ·æœ¬æ•°: {summary.get('total_samples', 0):,}")
        print(f"   å¹´å¹³å‡æ ·æœ¬æ•°: {summary.get('mean_samples_per_year', 0):,.0f}")
        
        # åˆ†ç±»æ€§èƒ½ç»“æœ
        class_df = pd.DataFrame(self.results['yearly_metrics']['classification'])
        if not class_df.empty:
            print(f"\n2. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ (2000-2019å¹´å¹³å‡Â±æ ‡å‡†å·®):")
            print(f"   å‡†ç¡®ç‡: {class_df['accuracy'].mean():.3f} Â± {class_df['accuracy'].std():.3f}")
            if 'precision' in class_df.columns:
                print(f"   ç²¾ç¡®ç‡: {class_df['precision'].mean():.3f} Â± {class_df['precision'].std():.3f}")
            if 'recall' in class_df.columns:
                print(f"   å¬å›ç‡: {class_df['recall'].mean():.3f} Â± {class_df['recall'].std():.3f}")
            if 'f1_score' in class_df.columns:
                print(f"   F1åˆ†æ•°: {class_df['f1_score'].mean():.3f} Â± {class_df['f1_score'].std():.3f}")
            if 'auc_roc' in class_df.columns:
                print(f"   AUC-ROC: {class_df['auc_roc'].mean():.3f} Â± {class_df['auc_roc'].std():.3f}")
        
        # æŠ•èµ„ç»„åˆæ€§èƒ½ç»“æœ
        port_df = pd.DataFrame([m for m in self.results['yearly_metrics']['portfolio'] if m is not None])
        if not port_df.empty:
            print(f"\n3. æŠ•èµ„ç»„åˆæ€§èƒ½æŒ‡æ ‡ (H-Lç­–ç•¥):")
            print(f"   ç­‰æƒé‡å¤æ™®æ¯”ç‡: {port_df['sharpe_ratio_ew'].mean():.3f} Â± {port_df['sharpe_ratio_ew'].std():.3f}")
            if 'sharpe_ratio_vw' in port_df.columns:
                print(f"   å¸‚å€¼æƒé‡å¤æ™®æ¯”ç‡: {port_df['sharpe_ratio_vw'].mean():.3f} Â± {port_df['sharpe_ratio_vw'].std():.3f}")
            print(f"   ç­‰æƒé‡H-Lå¹´å‡æ”¶ç›Š: {port_df.get('hl_return_mean_ew', pd.Series([0])).mean()*12:.3f}")
        
        # ç›¸å…³æ€§åˆ†æç»“æœ
        corr_metrics = self.results['pooled_metrics'].get('correlations', {})
        if corr_metrics:
            print(f"\n4. ç›¸å…³æ€§åˆ†æç»“æœ:")
            if 'spearman' in corr_metrics:
                spearman = corr_metrics['spearman']
                print(f"   æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°: {spearman.get('overall', 0):.3f}")
                if spearman.get('significant', False):
                    print("     (ç»Ÿè®¡æ˜¾è‘—)")
            
            if 'pearson' in corr_metrics:
                pearson = corr_metrics['pearson']
                print(f"   çš®å°”é€Šç›¸å…³ç³»æ•°: {pearson.get('overall', 0):.3f}")
                if pearson.get('significant', False):
                    print("     (ç»Ÿè®¡æ˜¾è‘—)")
            
            if 'information_coefficient' in corr_metrics:
                ic_info = corr_metrics['information_coefficient']
                print(f"   ä¿¡æ¯ç³»æ•°(IC)å‡å€¼: {ic_info.get('mean_ic', 0):.3f}")
                print(f"   ICä¿¡æ¯æ¯”ç‡: {ic_info.get('ic_ir', 0):.3f}")
                if ic_info.get('significant_5pct', False):
                    print("     (5%æ°´å¹³æ˜¾è‘—)")
                elif ic_info.get('significant_1pct', False):
                    print("     (1%æ°´å¹³æ˜¾è‘—)")
        
        # æ—¶é—´è¶‹åŠ¿åˆ†æ
        print(f"\n5. æ—¶é—´è¶‹åŠ¿åˆ†æ:")
        print(f"   å‡†ç¡®ç‡å¹´é™…è¶‹åŠ¿æ–œç‡: {summary.get('accuracy_trend', 0):.4f}")
        if 'sharpe_trend' in summary:
            print(f"   å¤æ™®æ¯”ç‡å¹´é™…è¶‹åŠ¿æ–œç‡: {summary['sharpe_trend']:.4f}")
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§
        if not port_df.empty and 'significance_ew' in port_df.columns:
            significant_years = port_df['significance_ew'].apply(
                lambda x: x.get('significant_5pct', False) if isinstance(x, dict) else False
            ).sum()
            total_years = len(port_df)
            print(f"\n6. ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ:")
            print(f"   5%æ˜¾è‘—æ€§æ°´å¹³æ˜¾è‘—å¹´ä»½: {significant_years}/{total_years} ({significant_years/total_years:.1%})")
        
        # æ¨¡å‹æ€§èƒ½è¯„ä¼°
        print(f"\n7. æ¨¡å‹æ€§èƒ½æ€»ä½“è¯„ä¼°:")
        accuracy_mean = summary.get('mean_accuracy', 0)
        if accuracy_mean > 0.55:
            print("   ğŸ“ˆ åˆ†ç±»æ€§èƒ½: ä¼˜ç§€ (å‡†ç¡®ç‡ > 55%)")
        elif accuracy_mean > 0.52:
            print("   ğŸ“Š åˆ†ç±»æ€§èƒ½: è‰¯å¥½ (å‡†ç¡®ç‡ > 52%)")
        else:
            print("   ğŸ“‰ åˆ†ç±»æ€§èƒ½: éœ€æ”¹è¿› (å‡†ç¡®ç‡ â‰¤ 52%)")
        
        sharpe_mean = summary.get('mean_sharpe_ew', 0)
        if sharpe_mean > 1.0:
            print("   ğŸ’¹ æŠ•èµ„ä»·å€¼: ä¼˜ç§€ (å¤æ™®æ¯”ç‡ > 1.0)")
        elif sharpe_mean > 0.5:
            print("   ğŸ“ˆ æŠ•èµ„ä»·å€¼: è‰¯å¥½ (å¤æ™®æ¯”ç‡ > 0.5)")
        else:
            print("   ğŸ“‰ æŠ•èµ„ä»·å€¼: æœ‰é™ (å¤æ™®æ¯”ç‡ â‰¤ 0.5)")
        
        print("\n" + "="*80)

    def calculate_comprehensive_correlations(self, df):
        """
        è®¡ç®—å®Œæ•´çš„ç›¸å…³æ€§æŒ‡æ ‡ï¼ˆå¤ç°åŸæ–‡è¡¨2ï¼‰
        åŒ…å«Spearmanå’ŒPearsonç›¸å…³ç³»æ•°
        """
        df_clean = df.dropna(subset=['cnn_prob_20d', 'Ret_20d'])
        
        if len(df_clean) < 2:
            return {}
        
        # 1. æ•´ä½“ç›¸å…³æ€§
        spearman_overall = df_clean['cnn_prob_20d'].corr(df_clean['Ret_20d'], method='spearman')
        pearson_overall = df_clean['cnn_prob_20d'].corr(df_clean['Ret_20d'], method='pearson')
        
        # 2. æ¨ªæˆªé¢ç›¸å…³æ€§ï¼ˆæŒ‰æ—¶é—´ç‚¹ï¼‰
        def cross_sectional_corr(group, method='spearman'):
            if len(group) < 2:
                return np.nan
            return group['cnn_prob_20d'].corr(group['Ret_20d'], method=method)
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„æ¨ªæˆªé¢ç›¸å…³æ€§
        spearman_cross = df_clean.groupby('Date').apply(cross_sectional_corr, method='spearman')
        pearson_cross = df_clean.groupby('Date').apply(cross_sectional_corr, method='pearson')
        
        # 3. ä¿¡æ¯ç³»æ•°(Information Coefficient)åˆ†æ
        ic_results = self.calculate_information_coefficient(df_clean)
        
        return {
            'spearman': {
                'overall': spearman_overall,
                'cross_sectional_mean': spearman_cross.mean(),
                'cross_sectional_std': spearman_cross.std(),
                'cross_sectional_ts': spearman_cross,
                'significant': abs(spearman_overall) > 0.05
            },
            'pearson': {
                'overall': pearson_overall,
                'cross_sectional_mean': pearson_cross.mean(),
                'cross_sectional_std': pearson_cross.std(),
                'cross_sectional_ts': pearson_cross,
                'significant': abs(pearson_overall) > 0.05
            },
            'information_coefficient': ic_results,
            'sample_size': len(df_clean)
        }

    def run_complete_analysis_pipeline(self):
        """æ‰§è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿"""
        try:
            # 1. æ•°æ®åŠ è½½é˜¶æ®µ
            print("="*80)
            print("é˜¶æ®µ1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
            print("="*80)
            self.load_all_years_data()
            
            if not self.yearly_data:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œåˆ†æç»ˆæ­¢")
                return None
            
            # 2. åˆ†å¹´æŒ‡æ ‡è®¡ç®—
            print("\n" + "="*80)
            print("é˜¶æ®µ2: åˆ†å¹´æŒ‡æ ‡è®¡ç®—")
            print("="*80)
            self.calculate_yearly_metrics()
            
            # 3. æ•´ä½“æŒ‡æ ‡è®¡ç®—
            print("\n" + "="*80)
            print("é˜¶æ®µ3: æ•´ä½“æŒ‡æ ‡è®¡ç®—")
            print("="*80)
            self.calculate_pooled_metrics()
            
            # 4. æ±‡æ€»ç»Ÿè®¡è®¡ç®—
            print("\n" + "="*80)
            print("é˜¶æ®µ4: æ±‡æ€»ç»Ÿè®¡åˆ†æ")
            print("="*80)
            self.calculate_cross_year_summary()
            
            # 5. ç»“æœä¿å­˜
            print("\n" + "="*80)
            print("é˜¶æ®µ5: ç»“æœä¿å­˜")
            print("="*80)
            self.save_comprehensive_results()
            
            # 6. å¯è§†åŒ–ç”Ÿæˆ
            print("\n" + "="*80)
            print("é˜¶æ®µ6: å¯è§†åŒ–ç”Ÿæˆ")
            print("="*80)
            self.create_comprehensive_visualizations()
            
            # 7. æŠ¥å‘Šç”Ÿæˆ
            print("\n" + "="*80)
            print("é˜¶æ®µ7: åˆ†ææŠ¥å‘Šç”Ÿæˆ")
            print("="*80)
            self.print_detailed_report()
            
            print("\nğŸ‰ åˆ†ææµç¨‹å®Œæˆï¼")
            return self.results
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None

    def calculate_pooled_metrics(self):
        """è®¡ç®—æ•´ä½“æŒ‡æ ‡"""
        if self.combined_data is not None:
            # ç›¸å…³æ€§æŒ‡æ ‡
            corr_metrics = self.calculate_comprehensive_correlations(self.combined_data)
            self.results['pooled_metrics']['correlations'] = corr_metrics
            
            # åˆ†ç±»æŒ‡æ ‡
            class_metrics = self.calculate_classification_metrics(self.combined_data)
            self.results['pooled_metrics']['classification'] = class_metrics

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # é…ç½®è·¯å¾„
    data_directory = "/workspace_ssd/wangjiang/monthly_60d_cnn_prob_baseline"  # æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
    output_directory = "./cnn_complete_analysis_results_60d"
    
    # åˆ›å»ºåˆ†æå®ä¾‹
    analyzer = CompleteCNNAnalysisPipeline(data_directory, output_directory)
    
    # æ‰§è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_analysis_pipeline()
    
    if results is not None:
        print("\n" + "="*80)
        print("åˆ†ææˆåŠŸå®Œæˆï¼")
        print("="*80)
        
        # ç”Ÿæˆç®€è¦æ€»ç»“
        summary = results['cross_year_summary']
        if summary:
            print(f"å…³é”®å‘ç°æ‘˜è¦:")
            print(f"â€¢ å¹³å‡åˆ†ç±»å‡†ç¡®ç‡: {summary.get('mean_accuracy', 0):.3f}")
            print(f"â€¢ å¹³å‡å¤æ™®æ¯”ç‡: {summary.get('mean_sharpe_ew', 0):.3f}")
            print(f"â€¢ è¦†ç›–æ ·æœ¬æ•°: {summary.get('total_samples', 0):,}")
            print(f"â€¢ æ—¶é—´èŒƒå›´: 2000-2019å¹´ ({summary.get('total_years', 0)}å¹´)")
        
        return results
    else:
        print("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")
        return None

if __name__ == "__main__":
    # æ‰§è¡Œä¸»åˆ†æ
    main_results = main()